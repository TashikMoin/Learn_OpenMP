// Array Summation Using Parallel Programming
#include<omp.h>
#include<stdio.h>
long int Array_Size = 1000000000 ;
int Array[1000000000] ;

int main()
{
    long int Final_Parallel_Sum = 0 , Sequential_Sum = 0 ;
    for( long int i = 0 ; i < Array_Size ; ++i )
    {
        Array[i] = 1 ; // Each Element = 1 so that final sum should be equal to Array_Size (100000000000)
    }
    double Starting_Time, Ending_Time ;
    printf("... Started Calculating The Sum Of Array ... \n") ;
    Starting_Time = omp_get_wtime() ; // omp_get_wtime() returns the current time
    #pragma omp parallel
    {
        int _TO , _FROM ; 
        int Thread_Id = omp_get_thread_num() ;
        int Total_Threads = omp_get_num_threads() ;
        /* 
        Declaring TO, FROM inside the parallel region because when we create variables inside the parallel region, the variables then are
        treated as private by default and if we declare them outside then they will be treated as shared. For every Thread, the TO and FROM
        Range that is the task division ranges ( summation division ranges ) are different. let say thread 0 sums from Array[0] to Array[25000000]
        and thread 1 sums from Array[25000000] to Array[50000000] and so on... that is why we created them private because each thread will be
        having To,From ranges different. If we calculate sum by using for loop with condition for( i = 0 ; i < size ; i++ ) then all the threads
        will run this loop size times and there is no task distribution because all the threads are calculating the sum individually that is why
        if we want to do the task distribution then we have to run a loop like this for(i = from ; i < to ; ++i ) in this case, from and to values
        will be different for every thread and all the threads will run different iterations of the same loops and hence the task is now distributed
        Let say if array size = 100 and total threads are '4' then task distribution will be done as,
           Thread # 0  ---> Array[0] ---   Array[24]
           Thread # 0  ---> Array[25] ---  Array[49]
           Thread # 0  ---> Array[50] ---  Array[74]
           Thread # 0  ---> Array[75] ---  Array[99]
        */
        /* By default my system's total threads are 4 because I have defined in the environmental variable to use only 4 threads by default 
           if it is not defined that the total threads to be used in a program are ' ' then use that default values of 4 by default */
        // Calculating TO And FROM for every thread
        _FROM = ( Array_Size / Total_Threads ) *  Thread_Id ;
        _TO = ( Array_Size / Total_Threads ) *  ( Thread_Id + 1 ) - 1 ; // Note: * has higher priority
        /* 
           Subtracting 1 becuase array index starts from 0 
           Note: The TO and FROM values will be different for each threads.
        */
        long int Private_Sum_By_Each_Thread = 0 ;
        printf("Total Threads : %d \n" , Total_Threads) ;
        printf("Thread # %d   FROM : %d     TO : %d   \n", Thread_Id , _FROM , _TO ); 
        for( long int i = _FROM ; i <= _TO ; ++i )
        {
            Private_Sum_By_Each_Thread += Array[i] ;
            /* We created this private sum variable here for every thread so that it can do summation without any inconsistency. If we would 
            have used the global/shared Final_Parallel_Sum variable here instead of Private_Sum_By_Each_Thread then we will not be able to see
            consistent output because the statement,
                Final_Parallel_Sum += Array[i] ; means that --> Final_Parallel_Sum = Final_Parallel_Sum + Array[i] ; multiple threads will
            be writing/reading values at the same time and that is the reason we will be getting a inconsistent output. However we could have
            used #pragma omp critical here and could have defined this statement under the critical block to avoid this problem but if we would 
            have done that then we will not be getting efficient output it will take too long and we will be getting x3 , x4 time of the sequential
            execution so this code will be useless the reason is, #pragma omp critical calls method called "Set and Get" that sets and reads the
            lock variable value so in this case if would have defined the statements under the critical block, this set and get method will be called
            Array_Size times suppose if the array size is 30000000000 so this method will be called 30000000000 due to which we will be getting
            more and more time so we created this Private_Sum_By_Each_Thread instead to avoid the race condition instead of the critical block.
            We defined the critical block to sum these private sum variables that can be seen below here now the set and get method will be called
            Total_Threads times or total private variable times means 4 times because here I am executing with 4 threads this means that 
            critical block now calls set and get only 4,5 times which is vvvvvvv less than the total array size times and we got efficient
            output as expected */
        }

        #pragma omp critical
        {
            Final_Parallel_Sum += Private_Sum_By_Each_Thread ;
        }
        
    }
    Ending_Time = omp_get_wtime() ;
    printf("\nThe Calculated Array Sum With Proper Parallelism = %lu With Total Time : %lf \n", Final_Parallel_Sum, (Ending_Time-Starting_Time) ) ;
    Starting_Time = omp_get_wtime() ;
    for( long int i = 0 ; i < Array_Size ; ++i )
    {
        Sequential_Sum += Array[i] ;
    }
    Ending_Time = omp_get_wtime() ;
    printf("\nThe Sequential Calculated Array Sum = %lu With Total Time : %lf \n", Sequential_Sum, (Ending_Time-Starting_Time) ) ;

}
